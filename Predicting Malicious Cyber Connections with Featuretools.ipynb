{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Malicious Cyber Connections\n",
    "<p style=\"margin:30px\">\n",
    "    <img style=\"display:inline; margin-right:50px\" width=50% src=\"https://www.featuretools.com/wp-content/uploads/2017/12/FeatureLabs-Logo-Tangerine-800.png\" alt=\"Featuretools\" />\n",
    "</p>\n",
    "\n",
    "The general setup for the problem is a common one: we have a single table of log lines recording Internet traffic between various sources. Traffic between a source and destination is labeled as malicious or clean in the dataset, and we'd like to be able to predict ahead of time if a future connection between a source and a destination will be malicious.\n",
    "\n",
    "We'll demonstrate an end-to-end workflow using this [Cybersecurity Dataset](). This notebook demonstrates a rapid way to predict whether a connection (defined as a source name/destination name pair) is malicious.\n",
    "\n",
    "\n",
    "## Highlights\n",
    "* Quickly make end-to-end workflow using log-line cybersecurity data\n",
    "* Find interesting automatically generated features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import CumMean, Percentile\n",
    "from featuretools.selection import remove_low_information_features\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Understanding the Data\n",
    "Here we load in the data and do a bit of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_df = pd.read_csv(\"CyberFLTenDays.csv\")\n",
    "cyber_df.index.name = \"log_id\"\n",
    "cyber_df.reset_index(inplace=True, drop=False)\n",
    "cyber_df['label'] = cyber_df['label'].map({'N': False, 'A': True}, na_action='ignore')\n",
    "\n",
    "# Sample down negative examples because very few positives\n",
    "# Can also do this after the feature engineering step (but doing it here reduces computation time)\n",
    "cyber_df_pos = cyber_df[cyber_df['label']]\n",
    "cyber_df_neg = cyber_df[~cyber_df['label']].sample(100000)\n",
    "cyber_df = pd.concat([cyber_df_pos, cyber_df_neg]).sort_values(['secs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an EntitySet\n",
    "To apply Deep Feature Synthesis we need to establish an `EntitySet` structure for our data. Since we're interested in predicting for combinations of \"src_name\" and \"dest_name\" (we call this pair a \"session\"), we need to create a separate normalized entity for \"sessions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: CyberLL\n",
       "  Entities:\n",
       "    log (shape = [100329, 7])\n",
       "    name_host_pairs (shape = [61929, 6])\n",
       "    sessions (shape = [19176, 4])\n",
       "  Relationships:\n",
       "    log.name_host_pair -> name_host_pairs.name_host_pair\n",
       "    name_host_pairs.session_id -> sessions.session_id"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(\"CyberLL\")\n",
    "# create an index column\n",
    "cyber_df[\"name_host_pair\"] = cyber_df[\"src_name\"].str.cat(\n",
    "                                [cyber_df[\"dest_name\"],\n",
    "                                 cyber_df[\"src_host\"],\n",
    "                                 cyber_df[\"dest_host\"]],\n",
    "                                sep=' / ')\n",
    "cyber_df[\"session_id\"] = cyber_df[\"src_name\"].str.cat(\n",
    "                                 cyber_df[\"dest_name\"],\n",
    "                                 sep=' / ')\n",
    "\n",
    "es.entity_from_dataframe(\"log\",\n",
    "                         cyber_df,\n",
    "                         index=\"log_id\",\n",
    "                         time_index=\"secs\")\n",
    "es.normalize_entity(base_entity_id=\"log\",\n",
    "                    new_entity_id=\"name_host_pairs\",\n",
    "                    index=\"name_host_pair\",\n",
    "                    additional_variables=[\"src_name\", \"dest_name\",\n",
    "                                          \"src_host\", \"dest_host\",\n",
    "                                          #\"src_pair\",\n",
    "                                          #\"dest_pair\",\n",
    "                                          \"session_id\",\n",
    "                                          \"label\"])\n",
    "es.normalize_entity(base_entity_id=\"name_host_pairs\",\n",
    "                    new_entity_id=\"sessions\",\n",
    "                    index=\"session_id\",\n",
    "                    additional_variables=[\"dest_name\", \"src_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate labels and associated cutoff times\n",
    "\n",
    "Featuretools can generate features for each session strictly before an associated cutoff time. We find these cutoff times in the process of computing labels. Labels are defined as follows:\n",
    "\n",
    "For a given session:\n",
    " * After seeing the same name/host pair N times\n",
    " * Predict L observations of this same session in the future\n",
    " * Where any connections from this session in a window of size W are malicious\n",
    " \n",
    "We will set N = 2 (number of observations to wait for), L = 2 (lead time), and W = 10 (prediction window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cutoffs(cyber_df, index_col, after_n_obs, lead, prediction_window):\n",
    "    window_start = after_n_obs + lead\n",
    "    window_end = window_start + prediction_window\n",
    "    grouped = cyber_df.groupby(index_col)[index_col].count()\n",
    "    grouped.name = \"count\"\n",
    "    min_obs = after_n_obs + lead + 1\n",
    "    enough_examples = grouped[grouped > min_obs].to_frame().reset_index()\n",
    "    enough_examples = cyber_df[cyber_df[index_col].isin(enough_examples[index_col])]\n",
    "    def get_label_and_cutoff(df):\n",
    "        cutoff = df.iloc[after_n_obs]\n",
    "        cutoff['label'] = df.iloc[window_start: window_end][\"label\"].any()\n",
    "        return cutoff\n",
    "    cutoffs = enough_examples.groupby(index_col)[[index_col, \"secs\", \"label\"]].apply(get_label_and_cutoff)\n",
    "    return cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = generate_cutoffs(cyber_df, \"session_id\", 2, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    4113\n",
       "True       35\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutoffs['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features using DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 54 features\n",
      "Elapsed: 00:48 | Remaining: 08:21 | Progress:   9%|â–‰         || Calculated: 364/4138 cutoff times"
     ]
    }
   ],
   "source": [
    "fm, fl = ft.dfs(entityset=es, target_entity=\"sessions\", cutoff_time=cutoffs,\n",
    "                cutoff_time_in_index=True,\n",
    "                verbose=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort indexes to line up cutoffs with feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = fm.reorder_levels(['time', 'session_id']).sort_index()\n",
    "cutoffs = cutoffs.set_index('secs', append=True).reorder_levels(['secs', 'session_id']).sort_index()\n",
    "fm['label'] = cutoffs['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encode categorical features and remove features with low information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_encoded, fl_encoded = ft.encode_features(fm, fl)\n",
    "fm_encoded, fl_encoded = remove_low_information_features(fm_encoded, fl_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Now that we have a feature matrix and associated labels, we can build a standard machine learning pipeline with a RandomForestClassifier\n",
    "\n",
    "First, split up the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(fm_encoded, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train\n",
    "y_train = X_train.pop('label')\n",
    "X_test = test\n",
    "y_test = X_test.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(missing_values='NaN', strategy=\"mean\", axis=0)\n",
    "scaler = StandardScaler()\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "model = Pipeline([(\"imputer\", imputer),\n",
    "                  (\"scaler\", scaler),\n",
    "                  (\"rf\", clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model, then score it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "    \n",
    "preds = model.predict(X_test)\n",
    "score = roc_auc_score(preds, y_test)\n",
    "print('ROC AUC Score: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the most important features\n",
    "according to the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_imp_feats = utils.feature_importances(X_train, clf, feats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
