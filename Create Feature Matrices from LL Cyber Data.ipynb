{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "from featuretools.primitives import CumMean, Percentile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_df = pd.read_csv(\"data/CyberFLTenDays.csv\").sample(10000)\n",
    "cyber_df.index.name = \"log_id\"\n",
    "cyber_df.reset_index(inplace=True, drop=False)\n",
    "cyber_df['label'] = cyber_df['label'].map({'N': False, 'A': True}, na_action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an EntitySet with many different entities\n",
    "\n",
    "Each entity is a different definition of \"connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: CyberLL\n",
       "  Entities:\n",
       "    dest_hosts [Rows: 932, Columns: 2]\n",
       "    dest_pairs [Rows: 7476, Columns: 4]\n",
       "    log [Rows: 10000, Columns: 7]\n",
       "    name_host_pairs [Rows: 8140, Columns: 5]\n",
       "    src_names [Rows: 5770, Columns: 2]\n",
       "    src_pairs [Rows: 7397, Columns: 4]\n",
       "    dest_names [Rows: 5785, Columns: 2]\n",
       "    src_hosts [Rows: 3567, Columns: 2]\n",
       "  Relationships:\n",
       "    log.name_host_pair -> name_host_pairs.name_host_pair\n",
       "    name_host_pairs.src_pair -> src_pairs.src_pair\n",
       "    src_pairs.src_name -> src_names.src_name\n",
       "    src_pairs.src_host -> src_hosts.src_host\n",
       "    name_host_pairs.dest_pair -> dest_pairs.dest_pair\n",
       "    dest_pairs.dest_name -> dest_names.dest_name\n",
       "    dest_pairs.dest_host -> dest_hosts.dest_host"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(\"CyberLL\")\n",
    "# create an index column\n",
    "cyber_df[\"name_host_pair\"] = cyber_df[\"src_name\"].str.cat(\n",
    "                                [cyber_df[\"dest_name\"],\n",
    "                                 cyber_df[\"src_host\"],\n",
    "                                 cyber_df[\"dest_host\"]],\n",
    "                                sep=' / ')\n",
    "cyber_df[\"src_pair\"] = cyber_df[\"src_name\"].str.cat(\n",
    "                                 cyber_df[\"src_host\"],\n",
    "                                 sep=' / ')\n",
    "cyber_df[\"dest_pair\"] = cyber_df[\"dest_name\"].str.cat(\n",
    "                                 cyber_df[\"dest_host\"],\n",
    "                                 sep=' / ')\n",
    "es.entity_from_dataframe(\"log\",\n",
    "                         cyber_df,\n",
    "                         index=\"log_id\",\n",
    "                         time_index=\"secs\")\n",
    "es.normalize_entity(base_entity_id=\"log\",\n",
    "                    new_entity_id=\"name_host_pairs\",\n",
    "                    index=\"name_host_pair\",\n",
    "                    additional_variables=[\"src_name\", \"dest_name\",\n",
    "                                          \"src_host\", \"dest_host\",\n",
    "                                          \"src_pair\",\n",
    "                                          \"dest_pair\",\n",
    "                                          \"label\"])\n",
    "es.normalize_entity(base_entity_id=\"name_host_pairs\",\n",
    "                    new_entity_id=\"src_pairs\",\n",
    "                    index=\"src_pair\",\n",
    "                    additional_variables=[\"src_name\", \"src_host\"])\n",
    "es.normalize_entity(base_entity_id=\"src_pairs\",\n",
    "                    new_entity_id=\"src_names\",\n",
    "                    index=\"src_name\")\n",
    "es.normalize_entity(base_entity_id=\"src_pairs\",\n",
    "                    new_entity_id=\"src_hosts\",\n",
    "                    index=\"src_host\")\n",
    "es.normalize_entity(base_entity_id=\"name_host_pairs\",\n",
    "                    new_entity_id=\"dest_pairs\",\n",
    "                    index=\"dest_pair\",\n",
    "                    additional_variables=[\"dest_name\", \"dest_host\"])\n",
    "es.normalize_entity(base_entity_id=\"dest_pairs\",\n",
    "                    new_entity_id=\"dest_names\",\n",
    "                    index=\"dest_name\")\n",
    "es.normalize_entity(base_entity_id=\"dest_pairs\",\n",
    "                    new_entity_id=\"dest_hosts\",\n",
    "                    index=\"dest_host\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to generate labels and cutoff times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cutoffs(cyber_df, index_col, after_n_obs, lead, prediction_window):\n",
    "    window_start = after_n_obs + lead\n",
    "    window_end = window_start + prediction_window\n",
    "    grouped = cyber_df.groupby(index_col)[index_col].count()\n",
    "    grouped.name = \"count\"\n",
    "    min_obs = after_n_obs + lead + 1\n",
    "    enough_examples = grouped[grouped > min_obs].to_frame().reset_index()\n",
    "    enough_examples = cyber_df[cyber_df[index_col].isin(enough_examples[index_col])]\n",
    "    def get_label_and_cutoff(df):\n",
    "        cutoff = df.iloc[after_n_obs]\n",
    "        cutoff['label'] = df.iloc[window_start: window_end][\"label\"].any()\n",
    "        return cutoff\n",
    "    cutoffs = enough_examples.groupby(index_col)[[index_col, \"secs\", \"label\"]].apply(get_label_and_cutoff)\n",
    "    return cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict after 3 observations\n",
    "after_n_obs = 3\n",
    "\n",
    "# predict 2 observations out\n",
    "lead = 2\n",
    "\n",
    "# predict if any malicious attacks in a 10-observation window\n",
    "window = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features for various types of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 807 features\n",
      "Elapsed: 04:06 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 11/11 chunks\n"
     ]
    }
   ],
   "source": [
    "# features on src_name\n",
    "cutoffs = generate_cutoffs(cyber_df, \"src_name\", after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"src_names\", cutoff_time=cutoffs, verbose=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 807 features\n",
      "Elapsed: 03:37 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 9/9 chunks\n"
     ]
    }
   ],
   "source": [
    "## features on src_host\n",
    "cutoffs = generate_cutoffs(cyber_df, \"src_host\", after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"src_hosts\", cutoff_time=cutoffs, verbose=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 807 features\n",
      "Elapsed: 04:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 11/11 chunks\n"
     ]
    }
   ],
   "source": [
    "## features on dest_name\n",
    "cutoffs = generate_cutoffs(cyber_df, \"dest_name\", after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"dest_names\", cutoff_time=cutoffs, verbose=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 807 features\n",
      "Elapsed: 03:16 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 8/8 chunks\n"
     ]
    }
   ],
   "source": [
    "## features on dest_host\n",
    "cutoffs = generate_cutoffs(cyber_df, \"dest_host\", after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"dest_hosts\", cutoff_time=cutoffs, verbose=True, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 121 features\n",
      "Elapsed: 2:04:42 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 6/6 chunks\n"
     ]
    }
   ],
   "source": [
    "# features on src_name/dest_name/src_host/dest_host\n",
    "cutoffs = generate_cutoffs(cyber_df, \"name_host_pair\", after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"name_host_pairs\", cutoff_time=cutoffs, verbose=True, max_depth=2, trans_primitives=[CumMean, Percentile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge entities together to access the index variables created in the process of normalizing\n",
    "merged = (es['log'].df\n",
    "                   .merge(es['name_host_pairs'].df)\n",
    "                   .merge(es['src_pairs'].df)\n",
    "                   .merge(es['dest_pairs'].df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 245 features\n",
      "Elapsed: 43:28 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 4/4 chunks\n"
     ]
    }
   ],
   "source": [
    "# features on src_name/src_host\n",
    "cutoffs = generate_cutoffs(merged, 'src_pair', after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"src_pairs\", cutoff_time=cutoffs, verbose=True, max_depth=2, trans_primitives=[CumMean, Percentile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 245 features\n",
      "Elapsed: 31:10 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 4/4 chunks\n"
     ]
    }
   ],
   "source": [
    "# features on dest_name/dest_host\n",
    "cutoffs = generate_cutoffs(merged, 'dest_pair', after_n_obs, lead, window)\n",
    "fm, fl = ft.dfs(entityset=es, target_entity=\"dest_pairs\", cutoff_time=cutoffs, verbose=True, max_depth=2, trans_primitives=[CumMean, Percentile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img src=\"https://www.featurelabs.com/wp-content/uploads/2017/12/logo.png\" alt=\"Featuretools\" />\n",
    "</p>\n",
    "\n",
    "Featuretools was created by the developers at [Feature Labs](https://www.featurelabs.com/). If building impactful data science pipelines is important to you or your business, please [get in touch](https://www.featurelabs.com/contact/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
